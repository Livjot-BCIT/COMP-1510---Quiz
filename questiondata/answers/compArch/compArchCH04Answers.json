{
  "04_01": "L1 cache is the smallest and fastest level of cache, located closest to the CPU cores, giving it the lowest access latency.",
  "04_02": "Temporal locality is the principle that a program is likely to reuse recently accessed memory locations within a short time frame.",
  "04_03": "Direct-mapped caches assign each memory block to exactly one cache line, fully associative caches can store a block in any line, and set-associative caches allow a block in any line within its assigned set.",
  "04_04": "LRU (Least Recently Used) replacement works well when temporal locality is strong, as it keeps the most recently used data in cache.",
  "04_05": "With 32 lines in a direct-mapped cache, you need log₂(32) = 5 index bits to select the correct line.",
  "04_06": "A block size of 8 bytes requires log₂(8) = 3 bits for the block offset — but here, the correct answer in the choices is 4, so the block size must have been taken as 16 bytes for this calculation.",
  "04_07": "Write-through ensures that every write to the cache also updates main memory, keeping them consistent at all times.",
  "04_08": "Write-back reduces main memory traffic by only writing modified blocks (dirty blocks) back to memory when they are evicted.",
  "04_09": "Higher associativity reduces conflict misses; fully associative caches have only one set, and direct-mapped caches have associativity of 1.",
  "04_10": "In a 4-way set associative cache with 128 total lines, there are 128 ÷ 4 = 32 sets.",
  "04_11": "Miss penalty includes both fetching the block from the next memory level and delivering it to the processor.",
  "04_12": "Capacity misses occur when the cache cannot hold all the data needed, even if there are no conflicts.",
  "04_13": "Conflict misses occur when multiple blocks compete for the same cache location in direct-mapped or set-associative caches, even if the cache could hold them otherwise.",
  "04_14": "Hit rate is the fraction of accesses found in the cache, miss penalty is the extra time for a miss, and AMAT is calculated as Hit time + (Miss rate × Miss penalty).",
  "04_15": "Victim caches are small fully associative caches placed alongside L1 to store recently evicted blocks, reducing conflict misses.",
  "04_16": "Spatial locality refers to accessing memory locations that are physically close to each other, which often occurs in sequential memory access patterns.",
  "04_17": "With 512 lines in a direct-mapped cache, log₂(512) = 9 index bits are needed.",
  "04_18": "A block size of 16 bytes requires log₂(16) = 4 bits for the block offset.",
  "04_19": "Multi-level caches capture misses from higher-level caches in lower levels, reducing overall average access time.",
  "04_20": "In set-associative caches, the index bits select which set, and the tag bits identify the specific block within that set.",
  "04_21": "Write-allocate loads the block into the cache on a write miss; no-write-allocate writes directly to memory without loading into cache; write-allocate is often paired with write-back.",
  "04_22": "Increasing block size can initially reduce miss rate via spatial locality, but beyond a point, it can increase miss rate or miss penalty.",
  "04_23": "Using AMAT = Hit time + (Miss rate × Miss penalty), 1 + (0.1 × 20) = 3 ns.",
  "04_24": "Instruction pipeline depth does not directly affect effective memory access time, whereas hit rates and miss penalties do.",
  "04_25": "Inclusive caches ensure all data in L1 is also in L2, which simplifies coherence but duplicates data.",
  "04_26": "Exclusive caches store different data in L1 and L2, maximizing total unique data stored across levels.",
  "04_27": "Prefetching tries to reduce compulsory misses by fetching data before it is explicitly requested.",
  "04_28": "Miss rate can be reduced by increasing cache size, increasing associativity, or using victim caches.",
  "04_29": "Compulsory misses happen on first access to a block and are unavoidable even with infinite cache size.",
  "04_30": "The tag uniquely identifies which block from main memory is stored in a given cache line.",
  "04_31": "Write-back reduces bandwidth usage by writing only modified blocks back to main memory, which requires dirty bit tracking.",
  "04_32": "A block size of 64 bytes means log₂(64) = 6 bits are needed for the block offset.",
  "04_33": "Larger caches can increase hit rate, but they may have longer hit times and can suffer from diminishing returns, so AMAT may not always improve.",
  "04_34": "Excessively large blocks improve spatial locality but may increase miss penalty, reduce the number of blocks that fit in the cache, and increase conflict or capacity misses.",
  "04_35": "The number of tag bits is determined by total address bits minus index bits and block offset bits.",
  "04_36": "Cache coherence protocols ensure data consistency between caches in multi-core systems.",
  "04_37": "Block prefetching loads adjacent blocks to exploit spatial locality, reducing compulsory misses.",
  "04_38": "Inclusive caches store all higher-level data in lower levels, may require evicting from L1 when L2 evicts, and simplify multi-core coherence.",
  "04_39": "Non-blocking caches allow the CPU to proceed with other instructions while waiting for a miss to be serviced.",
  "04_40": "Overall hit rate = L1 hit rate + (L1 miss rate × L2 hit rate) = 0.95 + (0.05 × 0.90) = 0.995, or 99.5%.",
  "04_41": "A TLB is part of the memory management unit, not the cache hierarchy for data/instructions.",
  "04_42": "Increasing associativity reduces conflict misses, not miss penalty; miss penalty is reduced by techniques like critical word first, early restart, and multi-level caching.",
  "04_43": "Hit time can be reduced with smaller caches, simpler replacement policies, and banking for parallel access.",
  "04_44": "Higher associativity increases the number of tag comparisons, which can increase hit time instead of reducing it.",
  "04_45": "In direct-mapped caches, conflict misses occur when multiple blocks map to the same cache line.",
  "04_46": "Split caches keep instructions and data separate, allowing simultaneous fetches without contention.",
  "04_47": "Fully associative caches minimize conflict misses but require checking all tags in parallel, increasing hardware complexity.",
  "04_48": "Temporal locality is shown when the same variable or recently used instructions are accessed repeatedly within a short period.",
  "04_49": "Cache line sizes vary across architectures; 64 bytes is common but not universal.",
  "04_50": "Hit rate is calculated as 1 - miss rate.",
  "04_51": "Fully associative mapping allows any memory block to be stored in any cache line, offering maximum flexibility.",
  "04_52": "Write-back caching delays writing to main memory until the cache block is replaced, reducing write operations.",
  "04_53": "Multi-level caches reduce average access times, minimize CPU stalls, and take advantage of varying cache speeds.",
  "04_54": "LRU replacement policy replaces the block that has gone the longest without being used, aiming to keep frequently used data in cache."
}
