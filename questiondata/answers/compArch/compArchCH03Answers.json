{
  "03_01": "ILP tries to overlap independent instructions in time, while DLP applies the same instruction to multiple data elements in parallel. ILP focuses on instruction scheduling; DLP focuses on data-wide operations.",
  "03_02": "Amdahl's Law: Speedup = 1 / [(1 - p) + (p / N)]. With p = 0.70 and N = 8 → 1 / (0.3 + 0.0875) ≈ 2.58×.",
  "03_03": "As N → ∞, speedup approaches 1 / (1 - p), meaning the serial fraction caps speedup. If p < 1, adding more cores eventually yields diminishing returns.",
  "03_04": "In a 5-stage pipeline, first instruction finishes in 5 cycles and each additional instruction takes 1 cycle. For 10 instructions: 5 + (10 - 1) = 14 cycles.",
  "03_05": "A data hazard happens when an instruction needs a result that a previous instruction hasn't yet produced (RAW dependency).",
  "03_06": "A structural hazard occurs when two stages simultaneously need the same hardware resource, like memory or an ALU.",
  "03_07": "Without hazards, 3 instructions take 7 cycles in a 5-stage pipeline. A single 1-cycle stall between I1 and I2 adds 2 cycles, totaling 9.",
  "03_08": "Data hazards can be mitigated with operand forwarding, compiler scheduling, or inserting stalls. Branch prediction addresses control hazards, not data hazards.",
  "03_09": "Branch prediction guesses the outcome of branches early, reducing control hazard penalties.",
  "03_10": "In an ideal pipeline with no hazards and balanced stages, CPI approaches 1.",
  "03_11": "In Hamming (7,4), parity bits are at positions 1, 2, and 4 — powers of two.",
  "03_12": "Parity bit P2 (position 2) covers positions 2, 3, 6, and 7 based on binary index bit patterns.",
  "03_13": "Hamming requirement: 2^r ≥ m + r + 1. For m = 11, r = 5 satisfies 32 ≥ 17.",
  "03_14": "Even parity makes total 1s even; odd parity makes them odd. Coverage sets don't change — only the value rule changes.",
  "03_15": "Placing D1=1, D2=0, D3=1, D4=1 into positions 3, 5, 6, 7 and computing even parity yields 0110011.",
  "03_16": "In Hamming codes, a non-zero syndrome means an error exists, and the binary value gives the bit position to flip.",
  "03_17": "Hamming (7,4) corrects all single-bit errors, detects some double-bit errors, and uses 3 parity + 4 data bits.",
  "03_18": "CRC calculation starts by appending k zeros to the data, where k is the degree of the generator polynomial.",
  "03_19": "Long division of 1101 with 1011 yields remainder 001, which is appended as the CRC.",
  "03_20": "Dividing 1101101011 by 11001 (x^4 + x^3 + 1) gives a remainder of 1111.",
  "03_21": "CRC detects burst errors more reliably than parity, Hamming corrects single-bit errors, and parity is weakest against bursts.",
  "03_22": "If the receiver divides (data + CRC) by the same generator and remainder is 0, no detectable error occurred.",
  "03_23": "L1 cache has the lowest latency in the memory hierarchy, faster than L3 or main memory.",
  "03_24": "Temporal locality means recently accessed memory locations are likely to be accessed again soon.",
  "03_25": "Direct-mapped maps one block to one line, fully associative allows any line, and set-associative maps to a set then any line in that set.",
  "03_26": "LRU replacement works well with strong temporal locality because it keeps recently used data in cache.",
  "03_27": "16 lines → log₂(16) = 4 index bits.",
  "03_28": "8-byte block → log₂(8) = 3 block offset bits. (Correction: The given correct answer is 4 bits if block size is 16 bytes.)",
  "03_29": "Write-through immediately writes changes from cache to main memory.",
  "03_30": "Write-back reduces main memory write traffic by writing only when a block is evicted.",
  "03_31": "Higher associativity reduces conflict misses, fully associative has one set, and direct-mapped has associativity 1.",
  "03_32": "64 lines in a 2-way set associative cache → 64 / 2 = 32 sets.",
  "03_33": "Miss penalty includes the time to fetch from the next level and deliver to the CPU.",
  "03_34": "Capacity misses happen when the working set is larger than the cache size.",
  "03_35": "Conflict misses occur when multiple blocks map to the same set or line despite enough total capacity.",
  "03_36": "Hit rate = fraction of accesses found in cache, miss penalty = extra time to service a miss, and AMAT = Hit time + (Miss rate × Miss penalty).",
  "03_37": "Victim caches are small fully associative caches used alongside L1 to catch lines evicted due to conflicts.",
  "03_38": "A page table stores the mapping between virtual and physical addresses.",
  "03_39": "A TLB caches recent address translations, is very fast, and a miss triggers a page table lookup.",
  "03_40": "Demand paging loads a page only when first accessed.",
  "03_41": "LRU is common in virtual memory replacement due to locality benefits.",
  "03_42": "Page faults happen when a page is not in memory, handled by the OS, and stall the CPU until resolved.",
  "03_43": "Thrashing happens when the system spends more time handling page faults than doing useful work.",
  "03_44": "Pipeline depth affects instruction execution, not memory access time directly. Cache/TLB hit rates and miss penalties do.",
  "03_45": "AMAT = 1 + (0.05 × (5 + (0.20 × 50))) = 6 ns.",
  "03_46": "AMAT improvement: from 1.8 ns (0.08 × 10 ns penalty) to 1.5 ns (0.05 × 10 ns penalty), a 0.3 ns gain.",
  "03_47": "Block offset bits = log₂(64) = 6 bits.",
  "03_48": "AMAT = 1 + (0.02 × 100) = 3 ns.",
  "03_49": "Larger caches can have longer hit times or cause more conflict misses for certain patterns, so AMAT doesn't always decrease.",
  "03_50": "Doubling block size reduces compulsory misses but can increase conflict misses due to fewer total blocks.",
  "03_51": "Task-level parallelism divides a program into independent tasks that can be executed at the same time.",
  "03_52": "Hamming codes can detect and correct single-bit errors, improving reliability in data transmission.",
  "03_53": "CRC uses polynomial division to detect errors, including burst errors, and is widely used in communication protocols.",
  "03_54": "Data hazards occur when an instruction depends on the result of a previous instruction still in the pipeline."
}
