{
  "01_01": "Computer organization focuses on how hardware and software work together inside a system, not just the design of software alone.",
  "01_02": "The microarchitecture level is directly above digital logic, defining how logic components are arranged to execute instructions.",
  "01_03": "Abstraction hides lower-level details so higher-level operations are easier to understand and use.",
  "01_04": "Hardware provides the physical components, while software tells those components what actions to perform.",
  "01_05": "Von Neumann architecture stores instructions and data in the same memory, allowing flexible program execution.",
  "01_06": "Environmental monitoring is not part of the computer hierarchy, which focuses on computational functions.",
  "01_07": "The control unit directs how data moves and how instructions are executed inside the CPU.",
  "01_08": "The stored-program concept lets one machine perform many tasks by loading different instructions into memory.",
  "01_09": "Microarchitecture defines the internal data paths and control signals that allow instructions to run.",
  "01_10": "Boolean gates form the lowest level of a computer, performing the simplest logical operations.",
  "01_11": "The CPU contains the control unit, ALU, and registers, which work together to process instructions.",
  "01_12": "Von Neumann systems store both data and instructions in the same physical memory.",
  "01_13": "Each level in the hierarchy—from logic gates to ISA—adds abstraction to manage complexity.",
  "01_14": "An ISA defines the exact set of instructions a CPU can execute, forming the interface between hardware and software.",
  "01_15": "Microprogramming implements a control unit by storing sequences of microinstructions.",
  "01_16": "Performance depends on both hardware capabilities and how efficiently software uses them.",
  "01_17": "Registers provide the fastest storage in a computer, located inside the CPU for immediate access.",
  "01_18": "Main memory holds active programs and data, but is slower than registers or cache.",
  "01_19": "Non-volatile memory retains data even when the system loses power.",
  "01_20": "Bus systems transfer data and instructions between CPU, memory, and I/O devices.",
  "01_21": "Control buses carry signals that coordinate operations among CPU, memory, and peripherals.",
  "01_22": "Data buses carry the actual binary data being moved between components.",
  "01_23": "Address buses carry location information so the CPU knows where to read or write data.",
  "01_24": "System performance can be limited by the slowest component in the data path, a bottleneck.",
  "01_25": "I/O devices require controllers to manage data exchange with the CPU and memory.",
  "01_26": "Interrupts let devices signal the CPU when they need attention, avoiding constant polling.",
  "01_27": "Direct Memory Access (DMA) allows devices to transfer data directly to memory without CPU intervention.",
  "01_28": "Firmware is low-level software stored in non-volatile memory that controls hardware.",
  "01_29": "Bootstrapping loads the initial operating system into memory so the system can start.",
  "01_30": "A clock signal coordinates all synchronous operations in a CPU.",
  "01_31": "Clock speed, measured in Hz, defines how many cycles a CPU can perform per second.",
  "01_32": "Higher clock speeds can improve performance but may increase heat and power use.",
  "01_33": "Parallelism allows multiple operations to occur simultaneously, improving performance.",
  "01_34": "Each hierarchy level handles specific tasks, abstracting complexity for higher levels.",
  "01_35": "Instruction formats define how operation codes and operands are arranged in binary form.",
  "01_36": "Fixed-length instructions simplify decoding, while variable-length instructions offer more flexibility.",
  "01_37": "Opcode specifies the operation, while operands specify the data or addresses used.",
  "01_38": "Control signals trigger specific hardware actions, such as reading memory or writing to registers.",
  "01_39": "The fetch-decode-execute cycle is the basic operational loop of the CPU.",
  "01_40": "Branch instructions change the normal sequential flow of program execution.",
  "01_41": "Conditional branches depend on the results of prior instructions.",
  "01_42": "Unconditional branches always change the flow regardless of conditions.",
  "01_43": "Subroutine calls allow code reuse and modular programming by jumping to and returning from a block of code.",
  "01_44": "Stack memory stores return addresses and local variables during subroutine calls.",
  "01_45": "Overflows occur when calculations exceed the maximum value storable in a given bit-width.",
  "01_46": "Signed and unsigned numbers differ in how they interpret the most significant bit.",
  "01_47": "Two's complement is the most common way to represent signed integers in binary.",
  "01_48": "Floating-point numbers allow representation of very large or small numbers using scientific notation in binary.",
  "01_49": "IEEE 754 is the standard for binary floating-point representation.",
  "01_50": "Normalization in floating-point ensures numbers are stored with a single non-zero digit before the decimal point.",
  "01_51": "Cache stores frequently used data closer to the CPU to reduce access time.",
  "01_52": "L1 cache is the smallest and fastest, located directly on the CPU core.",
  "01_53": "L2 cache is larger but slower than L1, often shared between cores.",
  "01_54": "L3 cache is even larger and shared across all cores in a CPU.",
  "01_55": "Hit rate measures how often requested data is found in cache.",
  "01_56": "Miss penalty is the delay caused when needed data isn't found in cache.",
  "01_57": "Write-through policy updates both cache and memory on each write.",
  "01_58": "Write-back policy updates memory only when cached data is replaced.",
  "01_59": "Cache associativity defines how cache lines map to main memory locations.",
  "01_60": "Direct-mapped cache assigns each memory block to exactly one cache line.",
  "01_61": "Fully associative cache allows any block to go into any line.",
  "01_62": "Set-associative cache is a compromise between direct-mapped and fully associative.",
  "01_63": "Pipeline stalls occur when an instruction can't proceed until a previous one finishes.",
  "01_64": "Data hazards happen when instructions depend on the results of earlier ones.",
  "01_65": "Control hazards occur from branch instructions that change program flow.",
  "01_66": "Structural hazards happen when hardware resources are over-committed.",
  "01_67": "Branch prediction guesses the outcome of branches to keep pipelines full.",
  "01_68": "Mispredictions cause pipeline flushes, wasting cycles.",
  "01_69": "Superscalar CPUs can execute multiple instructions per clock cycle.",
  "01_70": "Without special hardware, von Neumann CPUs can only fetch or execute at one time, not both simultaneously.",
  "01_71": "Pipelining increases CPU throughput by working on different stages of multiple instructions at once.",
  "01_72": "Instruction pipelines typically include fetch, decode, and execute stages for efficient processing.",
  "01_73": "Cache is placed close to the CPU to minimize data retrieval delays.",
  "01_74": "L1 cache is the smallest and fastest level, designed for immediate CPU access.",
  "01_75": "Increasing clock speed can raise heat and power usage, sometimes negating performance gains.",
  "01_76": "Moore's Law predicts transistor counts roughly doubling every two years, influencing computing power.",
  "01_77": "RISC uses fewer, simpler instructions for faster execution and easier pipeline design.",
  "01_78": "CISC has more complex instructions that can execute multi-step operations in one command.",
  "01_79": "Pipeline stalls can be caused by data, control, or structural hazards interrupting instruction flow.",
  "01_80": "Superscalar architecture boosts performance by running multiple instructions in parallel within one core.",
  "01_81": "Computer organization studies the physical hardware components and how they interact to execute instructions.",
  "01_82": "Computer architecture emphasizes system design principles and capabilities, focusing on what the system can do rather than exactly how it is built.",
  "01_83": "The stored-program concept means both instructions and data are stored in memory, fetched sequentially, and can share the same memory space.",
  "01_84": "Assembly is a low-level language closely tied to machine code, unlike Python, C++, or Java which are high-level languages."
}
